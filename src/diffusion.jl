using Random, Statistics, Zygote, NNlib

"Relu Activation function"
relu(x::AbstractArray) = max.(x, zero(eltype(x)))
relu(x::Number)        = max(x, zero(x))

"Glorot/Xavier uniform initialization: W·µ¢‚±º ~ U[-‚àö(6/(m+n)), ‚àö(6/(m+n))] via https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf"
glorot(m, n) = rand(Float32, m, n) .* (2f0*sqrt(6f0/(m+n))) .- sqrt(6f0/(m+n))
glorot_conv(w, h, c_in, c_out) = (rand(Float32, w, h, c_in, c_out) .* 2f0 .- 1f0) .* sqrt(6f0 / (w * h * (c_in + c_out)))

"Initialize fully convolutional network parameters for image-to-image noise prediction"
function conv_parameters(d)
    kernel_size = 3
    channels = [1, 16, 32, 16, 1]
    c_out = channels[2]
    W_alpha_bar = reshape(glorot(c_out, 1), 1, 1, c_out, 1)
    layers = []
    for i in 1:length(channels)-1
        push!(layers, (
            W=glorot_conv(kernel_size, kernel_size, channels[i], channels[i+1]),
            b=zeros(Float32, 1, 1, channels[i+1], 1)
        ))
    end
    return (layers=layers, W_alpha_bar=W_alpha_bar)
end

"forward process; ŒµÃÇ = œµŒ∏(xt,t)"
function predict(m, x, t, Œ±ÃÑ)
    H, W = size(x)
    h = reshape(x, H, W, 1, 1)
    padding = (size(first(m.layers).W, 1) - 1) √∑ 2

    # First layer with log snr Œ±ÃÑ injection
    h = conv(h, m.layers[1].W; pad=padding) .+ m.layers[1].b .+ m.W_alpha_bar .* snr(Œ±ÃÑ)[t]
    
    if length(m.layers) > 1
        h = relu(h)

        # Hidden layers
        for layer in m.layers[2:end-1]
            h = conv(h, layer.W; pad=padding) .+ layer.b
            h = relu(h)
        end
        
        # Final layer
        h = conv(h, m.layers[end].W; pad=padding) .+ m.layers[end].b
    end
    
    return reshape(h, H, W)
end

noise(x) = randn(eltype(x), size(x))
"The entire noise variance schedule via Œ≤_t = Œ≤_min + (Œ≤_max - Œ≤_min) * (t-1)/(T-1)"
noise_schedule(T; Œ≤min=1f-4, Œ≤max=0.02f0) = range(Œ≤min, Œ≤max; length=T)
"Entire signal variance schedule: Œ±_t = 1 - Œ≤_t"
signal_schedule(Œ≤::AbstractRange) = 1 .- Œ≤
"the total remaining signal variance is the cumprod of the signal_schedule"
remaining_signal(Œ±::AbstractRange) = cumprod(Œ±)
"Log Signal to Noise Ratio"
snr(Œ±ÃÑ) = log.(Œ±ÃÑ ./ (1 .- Œ±ÃÑ))
"Conditional marginal mean E[x‚Çú | x‚ÇÄ] for the forward diffusion process q(x‚Çú | x‚ÇÄ)"
marginal_mean(x, Œ±ÃÑ, t) = sqrt(Œ±ÃÑ[t]) .* x
"Conditional marginal noise for the forward diffusion marginal q(x‚Çú | x‚ÇÄ). This is the random Gaussian noise part added to the deterministic mean ‚àöŒ±ÃÑ‚Çú ¬∑ x‚ÇÄ."
marginal_noise(Œ±ÃÑ, t, Œµ) = sqrt(1-Œ±ÃÑ[t]).*Œµ
"Forward noise sample q(x_t | x_0) = sqrt(Œ±ÃÑ_t) * x_0 + sqrt(1 - Œ±ÃÑ_t) * Œµ, with Œµ ~ N(0, I)"
noised_sample(x0, Œ±ÃÑ, t, Œµ) = marginal_mean(x0, Œ±ÃÑ, t) .+ (sqrt(1-Œ±ÃÑ[t]) .* Œµ)
"Mean boxd Error (MSE) loss used for DDPM training: L‚Çõ·µ¢‚Çò‚Çö‚Çó‚Çë(Œ∏) := ùêÑ‚Çú,‚Çì‚ÇÄ,œµ ‚Äñœµ ‚àí œµŒ∏(‚àöŒ±ÃÑ‚Çú¬∑x‚ÇÄ + ‚àö(1‚àíŒ±ÃÑ‚Çú)¬∑œµ, t)‚Äñ¬≤"
loss(Œ∏, x, t, y, Œ±ÃÑ) = mean((y .- predict(Œ∏, x, t, Œ±ÃÑ)).^2)
"Stochastic Gradient Descent (SGD). m, ‚àá, Œ∑ are mlp_parameters, gradients, and learning rate respectively"
function sgd(m, ‚àá, Œ∑)
    layers = map(m.layers, ‚àá.layers) do layer, grad
        map((p, g) -> p .- Œ∑ .* g, layer, grad)
    end
    W_alpha_bar = m.W_alpha_bar .- Œ∑ .* ‚àá.W_alpha_bar
    (layers=layers, W_alpha_bar=W_alpha_bar)
end

"Performs one training step: adds noise x‚Çú = ‚àöŒ±ÃÑ‚Çú¬∑x‚ÇÄ + ‚àö(1‚àíŒ±ÃÑ‚Çú)¬∑Œµ and updates model by gradient of the loss (ŒµÃÇ, Œµ)"
function diffusion_step(m, x0, Œ±ÃÑ, T; t=rand(1:T), Œ∑=1e-3f0)
    Œµ  = noise(x0)
    xt = noised_sample(x0, Œ±ÃÑ, t, Œµ)
    (‚àá,) = gradient(Œ∏ -> loss(Œ∏, xt, t, Œµ, Œ±ÃÑ), m)
    sgd(m, ‚àá, Œ∑)
end

"Computes Œº‚Çú = (x‚Çú ‚àí (Œ≤‚Çú / ‚àö(1‚àíŒ±ÃÑ‚Çú))¬∑ŒµÃÇ) / ‚àöŒ±‚Çú for the reverse diffusion mean"
posterior_mean(x, ŒµÃÇ, Œ≤, Œ±, Œ±ÃÑ, t) = (x .- (Œ≤[t]/sqrt(1-Œ±ÃÑ[t])) .* ŒµÃÇ) ./ sqrt(Œ±[t])

"Draws a sample x‚Çú‚Çã‚ÇÅ ~ Œº + ‚àöŒ≤‚Çú ¬∑ N(0, I) from the reverse diffusion step"
latent(Œº, Œ≤, t, x) = Œº .+ sqrt(Œ≤[t]) .* randn(eltype(x), size(x))

"Generates ~x0 by iteratively sampling x‚Çú‚Çã‚ÇÅ = Œº‚Çú(x‚Çú, ŒµÃÇ) + ‚àöŒ≤‚Çú¬∑z for t = T,‚Ä¶,1, starting from x_T ~ N(0,I). "
function reverse_sample(m, Œ≤, Œ±, Œ±ÃÑ, T, d)
    H = W = isqrt(d)
    x = randn(Float32, H, W)
    Œº = similar(x)
    for t in T:-1:2
        ŒµÃÇ = predict(m, x, t, Œ±ÃÑ)
        Œº = posterior_mean(x, ŒµÃÇ, Œ≤, Œ±, Œ±ÃÑ, t)
        x = latent(Œº, Œ≤, t, x)
    end
    
    t = 1
    ŒµÃÇ = predict(m, x, t, Œ±ÃÑ)
    posterior_mean(x, ŒµÃÇ, Œ≤, Œ±, Œ±ÃÑ, t)
end

"Trains the diffusion model over the dataset by repeatedly applying one training step"
diffusion_train(model, Œ±ÃÑ, T, Œ∑, dataset) = foldl((m, x0) -> diffusion_step(m, x0, Œ±ÃÑ, T; Œ∑=Œ∑), dataset; init=model)
# "Trains for E epochs by folding `diffusion_train(model, Œ±ÃÑ, T, Œ∑, dataset)` over epochs: m‚Çë = foldl((m,_)->diffusion_train(m, Œ±ÃÑ, T, Œ∑, dataset), 1:E; init=model)"
# diffusion_train(model, Œ±ÃÑ, T, Œ∑, dataset, epochs) = foldl((m, _) -> diffusion_train(m, Œ±ÃÑ, T, Œ∑, dataset), 1:epochs; init=model)
function diffusion_train(model, Œ±ÃÑ, T, Œ∑, dataset, epochs)
    losses = Float32[]
    for _ in 1:epochs
        model = diffusion_train(model, Œ±ÃÑ, T, Œ∑, dataset)
        push!(losses, loss(model, xt_test, t_test, Œµ_test, Œ±ÃÑ))
    end
    display(plot(losses))
    return model
end

"Creates an h√ów zero matrix for a blank image"  
img(h, w) = zeros(Int, h, w)
"Paints a 3√ó3 block of 255s centered at (i, j) into an image (mutates)"  
addbox!(img, i, j) = (img[i-1:i+1, j-1:j+1] .= 255; img)
"Generates an h√ów image with a i√ój white box at (i, j)"  
box(h, w, i, j) = addbox!(img(h, w), i, j)
"Generates all possible unique boxes on a black background"
boxes(h, w) = [box(h, w, i, j) for i in 2:h-1 for j in 2:w-1]
"Scales an image (array) from [0,255] to [-1,1] via y = (2/255)*x - 1"
scale(img::Matrix) = (2 .* Float32.(img) ./ 255) .- 1
"Scales a vector of images by mapping `scale` over elements"
scale(imgs::AbstractVector) = map(scale, imgs)

# Below is just a scratch pad -- will delete after
using Test, Plots, BenchmarkTools

Random.seed!(42)
H,W = 16, 16
d = H*W
dataset = shuffle(scale(boxes(H, W)))

T = 1_000
Œ≤ = noise_schedule(T)
Œ± = signal_schedule(Œ≤)
Œ±ÃÑ = remaining_signal(Œ±)
model = conv_parameters(d)

# Why log.(Œ±ÃÑ ./ (1 .- Œ±ÃÑ) and not Œ±ÃÑ 
# plot(Œ±ÃÑ, label = "Œ±ÃÑ[t]") # Signal is compressed near 0 after 500 steps
# plot(Œ±ÃÑ ./ (1 .- Œ±ÃÑ), label = "SNR(t) = Œ±ÃÑ[t] / (1 - Œ±ÃÑ[t])") # Large dynamic range, but signal explodes on each end
# plot(log.(Œ±ÃÑ ./ (1 .- Œ±ÃÑ)), label = "log SNR(t) = log(Œ±ÃÑ[t] / (1 - Œ±ÃÑ[t]))") # Signal is compressed when SNR is low, and gives a large dynamic range afterwards

# Calculate loss before training on a sample
x0_test = rand(dataset)
Œµ_test = noise(x0_test)
t_test = rand(1:T)
xt_test = noised_sample(x0_test, Œ±ÃÑ, t_test, Œµ_test)
untrained_loss = loss(model, xt_test, t_test, Œµ_test, Œ±ÃÑ)

epochs = 10
model = diffusion_train(model, Œ±ÃÑ, T, 1f-1, shuffle(dataset), epochs)
# model = diffusion_train(model, Œ±ÃÑ, T, 1f-2, shuffle(dataset), epochs)
# model = diffusion_train(model, Œ±ÃÑ, T, 1f-3, shuffle(dataset), epochs)

# @code_warntype diffusion_train(model, Œ±ÃÑ, T, Œ∑, dataset, epochs)
# using BenchmarkTools
# @time model = diffusion_train(model, Œ±ÃÑ, T, Œ∑, dataset, epochs)

# # Calculate loss after training on the same sample

# @test trained_loss < untrained_loss

# # Reshape to 2-D and plot
heatmap(rand(dataset),
        color=:grays,
        aspect_ratio=:equal,
        title="Random generated box")

# Generate a 5√ó2 grid (10 samples) from the trained model
samples = [reverse_sample(model, Œ≤, Œ±, Œ±ÃÑ, T, d) for _ in 1:10]
plots = [heatmap(samples[i],
                 color=:grays,
                 aspect_ratio=1,
                 axis=false,
                 framestyle=:none,
                 xticks=false,
                 yticks=false,
                 colorbar=false) for i in 1:length(samples)]
plot(plots...;
     layout=(5,2),
     size=(300,500))
     
# H = W = isqrt(d)
# x = randn(Float32, H, W)
# Œº = similar(x)
# m = model
# Œºs = []

# for t in T:-1:1
#     ŒµÃÇ = predict(m, x, t, Œ±ÃÑ)
#     Œº = posterior_mean(x, ŒµÃÇ, Œ≤, Œ±, Œ±ÃÑ, t)
#     push!(Œºs,Œº)
#     x = latent(Œº, Œ≤, t, x)
# end

# N = length(Œºs)
# frames = 120
# idxs = round.(Int, N .- (N-1) .* exp.(-5.0 .* range(0,1,length=frames)))
# idxs = unique(idxs)
# idxs[end] = N
# anim = @animate for i in idxs
#     t = T - i + 1
#     heatmap(Œºs[i];
#             title = "t = $t",
#             color=:grays,
#             axis=false,
#             framestyle=:none,
#             xticks=false,
#             yticks=false,
#             aspect_ratio = :equal)
# end

# mp4(anim, "denoising.mp4", fps = 30)