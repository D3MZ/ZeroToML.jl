using Random, Statistics, Zygote, NNlib, Tullio, LoopVectorization

"Relu Activation function"
relu(x::AbstractArray) = max.(x, zero(eltype(x)))
relu(x::Number)        = max(x, zero(x))

"Glorot/Xavier uniform initialization: Wáµ¢â±¼ ~ U[-âˆš(6/(m+n)), âˆš(6/(m+n))] via https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf"
glorot(m, n) = rand(Float32, m, n) .* (2f0*sqrt(6f0/(m+n))) .- sqrt(6f0/(m+n))
"Glorot/Xavier for convolution"
glorot(w, h, c_in, c_out) = (rand(Float32, w, h, c_in, c_out) .* 2f0 .- 1f0) .* sqrt(6f0 / (w * h * (c_in + c_out)))

@kwdef struct DDPM
    Wâ‚ = glorot(3, 3, 1, 16)
    bâ‚ = zeros(Float32, 1, 1, 16, 1)
    Wâ‚‚ = glorot(3, 3, 16, 32)
    bâ‚‚ = zeros(Float32, 1, 1, 32, 1)
    Wâ‚ƒ = glorot(3, 3, 32, 16)
    bâ‚ƒ = zeros(Float32, 1, 1, 16, 1)
    Wâ‚„ = glorot(3, 3, 16, 1)
    bâ‚„ = zeros(Float32, 1, 1, 1, 1)
    Wâ‚œ = reshape(glorot(16, 1), 1, 1, 16, 1)
end

"model's forward process: ÎµÌ‚ = ÏµÎ¸(xt,t)"
function forward(m::DDPM, x, t, time_embedding)
    H, W = size(x)
    h = reshape(x, H, W, 1, 1)
    padding = (size(m.Wâ‚, 1) - 1) Ã· 2

    # Layer 1 with time embedding injection
    h = conv(h, m.Wâ‚; pad=padding) .+ m.bâ‚ .+ m.Wâ‚œ .* time_embedding[t]
    h = relu(h)

    # Layer 2
    h = conv(h, m.Wâ‚‚; pad=padding) .+ m.bâ‚‚
    h = relu(h)

    # Layer 3
    h = conv(h, m.Wâ‚ƒ; pad=padding) .+ m.bâ‚ƒ
    h = relu(h)
    
    # Layer 4 (Final layer)
    h = conv(h, m.Wâ‚„; pad=padding) .+ m.bâ‚„
    
    return reshape(h, H, W)
end

"Generates a box of the same type and size with random values"
noise(x) = randn(eltype(x), size(x))
"The entire noise variance schedule via Î²_t = Î²_min + (Î²_max - Î²_min) * (t-1)/(T-1)"
noise_schedule(T; Î²min=1f-4, Î²max=0.02f0) = range(Î²min, Î²max; length=T)
"Entire signal variance schedule: Î±_t = 1 - Î²_t"
signal_schedule(Î²::AbstractRange) = 1 .- Î²
"the total remaining signal variance is the cumprod of the signal_schedule"
remaining_signal(Î±::AbstractRange) = cumprod(Î±)
"Log Signal to Noise Ratio"
snr(Î±Ì„) = log.(Î±Ì„ ./ (1 .- Î±Ì„))
"Conditional marginal mean E[xâ‚œ | xâ‚€] for the forward diffusion process q(xâ‚œ | xâ‚€)"
marginal_mean(x, Î±Ì„, t) = sqrt(Î±Ì„[t]) .* x
"Conditional marginal noise for the forward diffusion marginal q(xâ‚œ | xâ‚€). This is the random Gaussian noise part added to the deterministic mean âˆšÎ±Ì„â‚œ Â· xâ‚€."
marginal_noise(Î±Ì„, t, Îµ) = sqrt(1-Î±Ì„[t]).*Îµ
"Forward noise sample q(x_t | x_0) = sqrt(Î±Ì„_t) * x_0 + sqrt(1 - Î±Ì„_t) * Îµ, with Îµ ~ N(0, I)"
noised_sample(xâ‚€, Î±Ì„, t, Îµ) = marginal_mean(xâ‚€, Î±Ì„, t) .+ (sqrt(1-Î±Ì„[t]) .* Îµ)
"Mean boxd Error (MSE) loss used for DDPM training: Lâ‚›áµ¢â‚˜â‚šâ‚—â‚‘(Î¸) := ð„â‚œ,â‚“â‚€,Ïµ â€–Ïµ âˆ’ ÏµÎ¸(âˆšÎ±Ì„â‚œÂ·xâ‚€ + âˆš(1âˆ’Î±Ì„â‚œ)Â·Ïµ, t)â€–Â²"
loss(Î¸::DDPM, x, t, y, time_embedding) = mean((y .- forward(Î¸, x, t, time_embedding)).^2)
"Stochastic Gradient Descent (SGD). m, âˆ‡, Î· are mlp_parameters, gradients, and learning rate respectively"
sgd!(m::DDPM, âˆ‡, Î·) = [getproperty(m, p) .-= Î· * getproperty(âˆ‡, p) for p in propertynames(m)]

"Performs one training step: adds noise xâ‚œ = âˆšÎ±Ì„â‚œÂ·xâ‚€ + âˆš(1âˆ’Î±Ì„â‚œ)Â·Îµ and updates model by gradient of the loss (ÎµÌ‚, Îµ)"
function step!(m::DDPM, xâ‚€, Î±Ì„, T, time_embedding; t=rand(1:T), Î·=1e-3f0)
    Îµ  = noise(xâ‚€)
    xt = noised_sample(xâ‚€, Î±Ì„, t, Îµ)
    (âˆ‡,) = gradient(Î¸ -> loss(Î¸, xt, t, Îµ, time_embedding), m)
    sgd!(m, âˆ‡, Î·)
    return m
end

"Computes Î¼â‚œ = (xâ‚œ âˆ’ (Î²â‚œ / âˆš(1âˆ’Î±Ì„â‚œ))Â·ÎµÌ‚) / âˆšÎ±â‚œ for the reverse diffusion mean"
posterior_mean(x, ÎµÌ‚, Î², Î±, Î±Ì„, t) = (x .- (Î²[t]/sqrt(1-Î±Ì„[t])) .* ÎµÌ‚) ./ sqrt(Î±[t])

"Draws a sample xâ‚œâ‚‹â‚ ~ Î¼ + âˆšÎ²â‚œ Â· N(0, I) from the reverse diffusion step"
latent(Î¼, Î², t, x) = Î¼ .+ sqrt(Î²[t]) .* randn(eltype(x), size(x))

"Generates ~x0 by iteratively sampling xâ‚œâ‚‹â‚ = Î¼â‚œ(xâ‚œ, ÎµÌ‚) + âˆšÎ²â‚œÂ·z for t = T,â€¦,0, starting from x_T ~ N(0,I). "
function reverse_sample(m::DDPM, Î², Î±, Î±Ì„, T, d, time_embedding)
    H = W = isqrt(d)
    x = randn(Float32, H, W)
    Î¼ = similar(x)
    for t in T:-1:2
        ÎµÌ‚ = forward(m, x, t, time_embedding)
        Î¼ = posterior_mean(x, ÎµÌ‚, Î², Î±, Î±Ì„, t)
        x = latent(Î¼, Î², t, x)
    end
    
    t = 1
    ÎµÌ‚ = forward(m, x, t, time_embedding)
    posterior_mean(x, ÎµÌ‚, Î², Î±, Î±Ì„, t)
end

function reverse_samples(m::DDPM, Î², Î±, Î±Ì„, T, d, time_embedding, N)                                                                                                                             
    samples = Vector{Matrix{Float32}}(undef, N)                                                                     
     Threads.@threads for i in eachindex(samples)                                                                   
        samples[i] = reverse_sample(m, Î², Î±, Î±Ì„, T, d, time_embedding)                                              
     end                                                                                                            
    return samples                                                                                                  
end 

"Trains the diffusion model over the dataset by repeatedly applying one training step"
train!(model::DDPM, Î±Ì„, T, Î·, dataset, time_embedding) = foldl((m, xâ‚€) -> step!(m, xâ‚€, Î±Ì„, T, time_embedding; Î·=Î·), dataset; init=model)
"Trains for E epochs by folding `train(model, Î±Ì„, T, Î·, dataset)` over epochs: mâ‚‘ = foldl((m,_)->train(m, Î±Ì„, T, Î·, dataset), 1:E; init=model)"
train!(model, Î±Ì„, T, Î·, dataset, epochs) = foldl((m, _) -> train!(m, Î±Ì„, T, Î·, dataset), 1:epochs; init=model)
