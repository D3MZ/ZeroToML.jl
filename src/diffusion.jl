using Random, Statistics, Zygote, NNlib

"Relu Activation function"
relu(x::AbstractArray) = max.(x, zero(eltype(x)))
relu(x::Number)        = max(x, zero(x))

"Glorot/Xavier uniform initialization: Wáµ¢â±¼ ~ U[-âˆš(6/(m+n)), âˆš(6/(m+n))] via https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf"
glorot(m, n) = rand(Float32, m, n) .* (2f0*sqrt(6f0/(m+n))) .- sqrt(6f0/(m+n))
glorot_conv(w, h, c_in, c_out) = (rand(Float32, w, h, c_in, c_out) .* 2f0 .- 1f0) .* sqrt(6f0 / (w * h * (c_in + c_out)))


"Initialize MLP mlp_parameters for dimension d -> d (noise prediction)"
function mlp_parameters(d, hidden_dims=[1024])
    c_out = 16
    kernel_size = 3
    H = W = isqrt(d)

    conv_layer = (
        W = glorot_conv(kernel_size, kernel_size, 1, c_out),
        b = zeros(Float32, 1, 1, c_out, 1)
    )

    dims = [H * W * c_out, hidden_dims..., d]
    layers = []
    for i in 1:length(dims)-1
        push!(layers, (W=glorot(dims[i+1], dims[i]), b=zeros(Float32, dims[i+1])))
    end
    return (conv_layer=conv_layer, layers=layers)
end

"forward process; ÎµÌ‚ = ÏµÎ¸(xt,t)"
function predict(m, x, t, Î±Ì„)
    H = W = isqrt(length(x))
    x_img = reshape(x, H, W, 1, 1)
    padding = (size(m.conv_layer.W, 1) - 1) Ã· 2
    h = conv(x_img, m.conv_layer.W; pad=padding) .+ m.conv_layer.b
    h = relu(h)
    h = reshape(h, :, 1)

    # MLP
    h = relu(m.layers[1].W * h .+ Î±Ì„[t] .+ m.layers[1].b)
    for layer in m.layers[2:end-1]
        h = relu(layer.W * h .+ layer.b)
    end
    m.layers[end].W * h .+ m.layers[end].b
end

noise(x) = randn(eltype(x), size(x))
"The entire noise variance schedule via Î²_t = Î²_min + (Î²_max - Î²_min) * (t-1)/(T-1)"
noise_schedule(T; Î²min=1f-4, Î²max=0.02f0) = range(Î²min, Î²max; length=T)
"Entire signal variance schedule: Î±_t = 1 - Î²_t"
signal_schedule(Î²::AbstractRange) = 1 .- Î²
"the total remaining signal variance is the cumprod of the signal_schedule"
remaining_signal(Î±::AbstractRange) = cumprod(Î±)
"Conditional marginal mean E[xâ‚œ | xâ‚€] for the forward diffusion process q(xâ‚œ | xâ‚€)"
marginal_mean(x, Î±Ì„, t) = sqrt(Î±Ì„[t]) .* x
"Conditional marginal noise for the forward diffusion marginal q(xâ‚œ | xâ‚€). This is the random Gaussian noise part added to the deterministic mean âˆšÎ±Ì„â‚œ Â· xâ‚€."
marginal_noise(Î±Ì„, t, Îµ) = sqrt(1-Î±Ì„[t]).*Îµ
"Forward noise sample q(x_t | x_0) = sqrt(Î±Ì„_t) * x_0 + sqrt(1 - Î±Ì„_t) * Îµ, with Îµ ~ N(0, I)"
noised_sample(x0, Î±Ì„, t, Îµ) = marginal_mean(x0, Î±Ì„, t) .+ (sqrt(1-Î±Ì„[t]) .* Îµ)
"Mean Squared Error (MSE) loss used for DDPM training: Lâ‚›áµ¢â‚˜â‚šâ‚—â‚‘(Î¸) := ð„â‚œ,â‚“â‚€,Ïµ â€–Ïµ âˆ’ ÏµÎ¸(âˆšÎ±Ì„â‚œÂ·xâ‚€ + âˆš(1âˆ’Î±Ì„â‚œ)Â·Ïµ, t)â€–Â²"
loss(Î¸, x, t, y, Î±Ì„) = mean((y .- predict(Î¸, x, t, Î±Ì„)).^2)
"Stochastic Gradient Descent (SGD). m, âˆ‡, Î· are mlp_parameters, gradients, and learning rate respectively"
function sgd(m, âˆ‡, Î·)
    layers = map(m.layers, âˆ‡.layers) do layer, grad
        map((p, g) -> p .- Î· .* g, layer, grad)
    end
    conv_layer = map(m.conv_layer, âˆ‡.conv_layer) do p, g
        p .- Î· .* g
    end
    (conv_layer=conv_layer, layers=layers)
end

"Performs one training step: adds noise xâ‚œ = âˆšÎ±Ì„â‚œÂ·xâ‚€ + âˆš(1âˆ’Î±Ì„â‚œ)Â·Îµ and updates model by gradient of the loss (ÎµÌ‚, Îµ)"
function diffusion_step(m, x0, Î±Ì„, T; t=rand(1:T), Î·=1e-3f0)
    Îµ  = noise(x0)
    xt = noised_sample(x0, Î±Ì„, t, Îµ)
    (âˆ‡,) = gradient(Î¸ -> loss(Î¸, xt, t, Îµ, Î±Ì„), m)
    sgd(m, âˆ‡, Î·)
end

"Computes Î¼â‚œ = (xâ‚œ âˆ’ (Î²â‚œ / âˆš(1âˆ’Î±Ì„â‚œ))Â·ÎµÌ‚) / âˆšÎ±â‚œ for the reverse diffusion mean"
posterior_mean(x, ÎµÌ‚, Î², Î±, Î±Ì„, t) = (x .- (Î²[t]/sqrt(1-Î±Ì„[t])) .* ÎµÌ‚) ./ sqrt(Î±[t])

"Draws a sample xâ‚œâ‚‹â‚ ~ Î¼ + âˆšÎ²â‚œ Â· N(0, I) from the reverse diffusion step"
latent(Î¼, Î², t, x) = Î¼ .+ sqrt(Î²[t]) .* randn(eltype(x), size(x))

"Generates ~x0 by iteratively sampling xâ‚œâ‚‹â‚ = Î¼â‚œ(xâ‚œ, ÎµÌ‚) + âˆšÎ²â‚œÂ·z for t = T,â€¦,1, starting from x_T ~ N(0,I). "
function reverse_sample(m, Î², Î±, Î±Ì„, T, d)
    x = randn(Float32, d)
    Î¼ = similar(x)
    for t in T:-1:2
        ÎµÌ‚ = predict(m, x, t, Î±Ì„)
        Î¼ = posterior_mean(x, ÎµÌ‚, Î², Î±, Î±Ì„, t)
        x = latent(Î¼, Î², t, x)
    end
    
    t = 1
    ÎµÌ‚ = predict(m, x, t, Î±Ì„)
    posterior_mean(x, ÎµÌ‚, Î², Î±, Î±Ì„, t)
end

"Trains the diffusion model over the dataset by repeatedly applying one training step"
diffusion_train(model, Î±Ì„, T, Î·, dataset) = foldl((m, x0) -> diffusion_step(m, x0, Î±Ì„, T; Î·=Î·), dataset; init=model)
"Trains for E epochs by folding `diffusion_train(model, Î±Ì„, T, Î·, dataset)` over epochs: mâ‚‘ = foldl((m,_)->diffusion_train(m, Î±Ì„, T, Î·, dataset), 1:E; init=model)"
diffusion_train(model, Î±Ì„, T, Î·, dataset, epochs) = foldl((m, _) -> diffusion_train(m, Î±Ì„, T, Î·, dataset), 1:epochs; init=model)

"Generates a square of 255s against a 0s background"
function square(h, w)
    d = h * w
    img = zeros(Int, h, w)
    i = rand(4:12); j = rand(4:12)
    img[i-1:i+1, j-1:j+1] .= 255
    return reshape(img, d)
end

"Scales an image from [0, 255] to [-1, 1]"
scale(img) = (2.0f0 .* Float32.(img) ./ 255.0f0) .- 1.0f0

using Test
# Below is just a scratch pad -- will delete after
Random.seed!(42)
H,W = 16, 16
d = H*W
dataset = [scale(square(H, W)) for _ in 1:100]

T = 1_000
Î² = noise_schedule(T)
Î± = signal_schedule(Î²)
Î±Ì„ = remaining_signal(Î±)
model = mlp_parameters(d, [512])

# Calculate loss before training on a sample
x0_test = scale(square(H, W))
Îµ_test = noise(x0_test)
t_test = rand(1:T)
xt_test = noised_sample(x0_test, Î±Ì„, t_test, Îµ_test)
untrained_loss = loss(model, xt_test, t_test, Îµ_test, Î±Ì„)

Î· = 1f-1
@time model = diffusion_train(model, Î±Ì„, T, Î·, dataset)
# epochs = 1
# @code_warntype diffusion_train(model, Î±Ì„, T, Î·, dataset, epochs)
# using BenchmarkTools
# @benchmark diffusion_train(model, Î±Ì„, T, Î·, dataset, epochs)
# @time model = diffusion_train(model, Î±Ì„, T, Î·, dataset, epochs)

# # Calculate loss after training on the same sample
trained_loss = loss(model, xt_test, t_test, Îµ_test, Î±Ì„)
@info "untrained_loss=$(untrained_loss) trained_loss=$(trained_loss)"
@test trained_loss < untrained_loss

using Plots

# # Reshape to 2-D and plot
heatmap(reshape(first(dataset), H, W),
        color=:grays,
        aspect_ratio=:equal,
        title="Random generated square")

# Generate a 5Ã—2 grid (10 samples) from the trained model
samples = [reshape(reverse_sample(model, Î², Î±, Î±Ì„, T, d), H, W) for _ in 1:10]
plots = [heatmap(samples[i],
                 color=:grays,
                 aspect_ratio=1,
                 axis=false,
                 framestyle=:none,
                 xticks=false,
                 yticks=false,
                 colorbar=false) for i in 1:length(samples)]
plot(plots...;
     layout=(5,2),
     size=(300,500))
